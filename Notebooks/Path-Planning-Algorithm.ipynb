{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>The Path Planning Algorithm (PPA)</center>\n",
    "    Luis Robaina : luis.f.robaina@nasa.gov\n",
    "    Ames Reseach Center Intern, Fall 2020.\n",
    "    Mentored by: Gilbert Wu, Tech Lead Modeling and Simulation.\n",
    "                 gilbert.wu@nasa.gov\n",
    "    \n",
    "    Last Update: Nov 8, 2020.\n",
    "    \n",
    "**Note: This Notebook is for documentation purposes, to train/test the PPA please use the source code and refer to  README for instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "As we find more potential applications to Unmanned Aircraft Systems (UAS), the ability to safely operate them has become an important challenge. The demand for UAS has steadily increased during the last 10 years. Our capacity to safely integrate these unmanned aircraft with manned aircraft in the airspace requires the ability to equip them with both collision avoidance (CA) and detect and avoid (DAA) systems capabilities. DAA systems use real-time telemetry to detect other aircraft in their vicinity and provide the UAS operator with the necessary guidance to be aware of collision risk and to collaborate with air traffic control to address the risk. It is also necessary to identify potential conflict-free trajectories so that the UAS can recover its flight path. \n",
    "Significant efforts are currently in progress to find algorithms that efficiently replicate the functionality of DAA systems. Previous efforts to construct a robust DAA system have been based on geometric formulations as well as Deep Learning approaches. The approach taken in this research project is known as Reinforcement Learning (RL) — A form of Machine Learning (ML). This approach aims to construct a proof-of-concept RL model able to recommend robust maneuver strategies for a UAS in conflict with another aircraft under simplified conditions. Compared to other formulations of this problem, this approach does not require labeled training data — commonly used in supervised ML — or complex geometric algorithms to achieve its objectives. This approach lets the UAS interact with a simulated environment to generate its training data: Rewarding sequence of maneuvers that create successful trajectories and penalizing those maneuvers that generate some form of conflict. Running numerous simulations over a set of different encounter geometries with this RL process, the UAS can find a trajectory without conflicts to the destination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Sample Conflict Resolution Strategy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample Result](Images/2.jpg \"PPA Encounter Solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>PPA Source Code Structure</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Project Modules Graph](Images/PPAGraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abbreviations and Terms Used\n",
    "- **PPA** : Path Planning Algorithm\n",
    "- **Ownship** : The aircraft that this algorithm controls, aka \"The agent\"\n",
    "- **Intruder** : The aircraft that the ownship will avoid conflict with. This algorithm does not control the intruder\n",
    "- **LODWC** : Lost of Distance \"Well Clear\"\n",
    "- **CPA** : Closest Point of Approach\n",
    "- **MCTS** : Monte Carlo Tree Search\n",
    "- **The Model** : Refers to the file that will store the results of our training and will be used to test. The model answers the following question: I am at this discrete state, what is the best action to take? The model is created during training and loaded during testing.\n",
    "- **Q Value** : \"The expected reward\". In the context of MCTS this is the expected discounted sum of rewards that one can expect starting from a given node on the tree down its sub-tree. In the context of the model this is the expected reward for taking an action from a given discrete state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modules: Remember to run pip install before you run the source code :)\n",
    "import math\n",
    "from random import random\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import pandas as pd\n",
    "import pickle as p\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>PPA Algorithm Constants</center>\n",
    "\n",
    "<center>Source Code Module: <i>global_constants.py<i></center>\n",
    "    \n",
    "**<u>Notes About Global Constants:</u>**\n",
    "    \n",
    "- **TIME_INCREMENT**: Every action is assumed to take <i>TIME_INCREMENT</i> senconds once it is selected by the agent (During training), Directly affects the performance of the algorithm in terms of training time and coverage of the state. Small time increments during training requires more training time to cover the state space.\n",
    "- **TEST_TIME_INCREMENT**: Similar to TIME_INCREMENT but used only when testing the PPA (Not during training).\n",
    "- **GAMMA**: A discount factor for future actions: Actions that lead to a reward after many steps are valued less than the same rewards that come by fewer steps. [Article about Gamma](https://towardsdatascience.com/penalizing-the-discount-factor-in-reinforcement-learning-d672e3a38ffe)\n",
    "- **DESTINATION_STATE**: The coordinates of the destination state for the problem definition, for all encounters we make the destination be the origin of the 2D coordinate system and place the ownship on the negative y axis at some distance.\n",
    "- **DESTINATION_STATE_REWARD**: This is the only reward our agent gets, if a sequence of actions leads to the agent arriving to the destination then those actions are rewarded by +1.\n",
    "- **ABANDON_STATE_REWARD**: Penalty if a sequence of actions lead to the agent being too far from the destination.\n",
    "- **ABANDON_STATE_ERROR**: If the agent is at a distance greated than or equal to the value of this constant we consider it an Abandon State and penalize it with ABANDON_STATE_REWARD.\n",
    "- **LODWC_REWARD**: Penalty if a sequence of actions lead to the agent being too close to the intruder (LODWC).\n",
    "- **DWC_DIST**: If this minimum distance between ownship and intruder is violated we consider it a LODWC.  \n",
    "- **TURN_ACTION_REWARD**: Penalty for every turn action the agent selects: Turning is resource inneficient, if a conflict can be avoided simply by going straight then avoid turnig or will penalize the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Algorithm's Constants:\n",
    "# Refer to README for more details about rewards/penalties.\n",
    "TIME_INCREMENT = 10.0           # Seconds that each action will runs for (During training)\n",
    "TEST_TIME_INCREMENT = 10.0      # Seconds that each action will run for (During testing)\n",
    "DESTINATION_STATE = [0, 0]      # Coordinates of the destination.\n",
    "\n",
    "DESTINATION_STATE_REWARD = 1.0  # Reward for reaching the destination.\n",
    "ABANDON_STATE_REWARD = -0.5     # Negative reward (a penalty) if ownship reaches state too far from destination.\n",
    "# We define Lost of Well Clear if the distance between aircraft is less than 2200 ft.\n",
    "LODWC_REWARD = -0.3             # Negative reward (penalty) for Lost of Well Clear.\n",
    "TURN_ACTION_REWARD = -0.00001   # Negative reward (penalty) for every turn action.\n",
    "\n",
    "# Final State Constants:\n",
    "DWC_DIST = 2200                 # (ft) Well Clear distance.\n",
    "\n",
    "DESTINATION_DIST_ERROR = 500    # (ft) Max distance from the destination which is considered Destination Reached.\n",
    "\n",
    "ABANDON_STATE_ERROR = 30000     # (ft) Distance which if exceed results on an abandon state.\n",
    "\n",
    "# Set of actions the ownship can take.\n",
    "ACTIONS = {                     # Actions are in degrees per second.\n",
    "    'LEFT': -5,\n",
    "    'NO_TURN': 0,\n",
    "    'RIGHT': 5\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Maximum number of MCTS iterations that can run for a given encounter. \n",
    "Each iteration of MCTS includes: selection, expansion, simulation.\n",
    "Refer to MCTS.py for more details.\n",
    "\"\"\"\n",
    "MCTS_ITERATIONS = 10000\n",
    "# Every MCTS_CUT iterations try to construct a trajectory. If it is successful then move to the next training encounter.\n",
    "# If a cut is not desired set MCTS_CUT = 1. Then every MCTS will go for MCTS_ITERATIONS.\n",
    "MCTS_CUT = 1000\n",
    "\n",
    "UCB1_C = 2                      # UCB1 Exploration term.\n",
    "GAMMA = 0.9                     # Discount Factor.\n",
    "\n",
    "# Max number of actions that can be taken when simulating for Performance.\n",
    "EPISODE_LENGTH = None\n",
    "\n",
    "# Directory Paths:\n",
    "# Set of Training Encounters.\n",
    "TRAINING_SET = 'Test Files/Test_Encounter_Geometries.csv'\n",
    "# Directory where each encounter description/trajectory will be placed.\n",
    "TEST_RESULTS_PATH = os.getcwd() + '/Test Results'\n",
    "\n",
    "# Conversion Factors\n",
    "NMI_TO_FT = 6076.12\n",
    "HR_TO_SEC = 3600\n",
    "\n",
    "# For state discretization purposes:\n",
    "MIN_DISTANCE = 0              # (ft).\n",
    "MAX_DISTANCE = 60761          # (ft) equivalent to 10 Nautical Miles.\n",
    "\n",
    "MIN_SPEED = 0                 # (ft/sec).\n",
    "MAX_SPEED = 287               # About 170 knot (ft/sec).\n",
    "\n",
    "MIN_ANGLE = -180              # (deg).\n",
    "MAX_ANGLE = 180               # (deg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Load a Set of Test Encounter Geometries</center>\n",
    "<a id='encounter_properties'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the way the encounter geometries are described with this sample training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>time_to_CPA_sec</th>\n",
       "      <th>destination_time_after_CPA_sec</th>\n",
       "      <th>OIF_CPA</th>\n",
       "      <th>CPA_distance_ft</th>\n",
       "      <th>v_o_kts</th>\n",
       "      <th>v_i_kts</th>\n",
       "      <th>int_rel_heading_deg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>170</td>\n",
       "      <td>-180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Run  time_to_CPA_sec  destination_time_after_CPA_sec  OIF_CPA  \\\n",
       "0      0               90                              30    False   \n",
       "1      1               90                              30    False   \n",
       "2      2               90                              30     True   \n",
       "3      3               90                              30     True   \n",
       "4      4               90                              30     True   \n",
       "..   ...              ...                             ...      ...   \n",
       "275  275               90                              30    False   \n",
       "276  276               90                              30    False   \n",
       "277  277               90                              30     True   \n",
       "278  278               90                              30     True   \n",
       "279  279               90                              30     True   \n",
       "\n",
       "     CPA_distance_ft  v_o_kts  v_i_kts  int_rel_heading_deg  \n",
       "0               2000      100        0                    0  \n",
       "1               1000      100        0                    0  \n",
       "2                  0      100        0                    0  \n",
       "3               1000      100        0                    0  \n",
       "4               2000      100        0                    0  \n",
       "..               ...      ...      ...                  ...  \n",
       "275             2000      100      170                 -180  \n",
       "276             1000      100      170                 -180  \n",
       "277                0      100      170                 -180  \n",
       "278             1000      100      170                 -180  \n",
       "279             2000      100      170                 -180  \n",
       "\n",
       "[280 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENCOUNTERS_GEOMETRIES = pd.read_csv(TRAINING_SET, header = 0)\n",
    "ENCOUNTERS_GEOMETRIES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Encounter Geometry Features\n",
    "- **Run**: Index of a given training encounter (Must start at 0).\n",
    "- **Time to CPA (sec)** : The number of seconds that the ownship takes to reach the closest point of approximation (CPA) moving at a constant velocity without any manouvers.\n",
    "(The \"Closest Point of Approach\" refers to the positions at which two dynamically moving objects reach their closest possible distance)\n",
    "- **Destination time after CPA**: The number of seconds the ownship takes to reach the destination point after the CPA.\n",
    "\n",
    "**(The sum of Time to CPA and Destination time after CPA allows us to place the ownship on the exact coordinates such that it takes the sum of these times to reach destination point at the given speed without any manouvers)**\n",
    "- **OIF_CPA**: True if the ownship passes in front of  the intruder at CPA, false otherwise.\n",
    "- **CPA_distance_ft**: Distance between the ownship and intruder at CPA in feet.\n",
    "- **v_o_kts and v_i_kts**: Speed of the ownship and intruder respectively in Knot: The knot is a unit of speed equal to one nautical mile per hour.\n",
    "- **int_rel_heading_deg**: Intruder's heading relative to the ownship's initial heading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute State Definition\n",
    "Defines the state of the ownship-intruder system in continuous 2D coordinates.\n",
    "<a id='absolute_state'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    # Define a state object type.\n",
    "    def __init__(self, ownship_pos, intruder_pos, ownship_vel, intruder_vel):\n",
    "\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            All the features of a State object are numpy arrays.\n",
    "        \"\"\"\n",
    "        self.ownship_pos = ownship_pos      # [x,y] (ft)\n",
    "        self.intruder_pos = intruder_pos    # [x,y] (ft)\n",
    "        self.ownship_vel = ownship_vel      # [v_x,v_y] (ft/sec)\n",
    "        self.intruder_vel = intruder_vel    # [v_x,v_y] (ft/sec)\n",
    "\n",
    "    def get_distance(self):                 # Distance between the aircraft in ft.\n",
    "        return LA.norm(self.ownship_pos - self.intruder_pos)\n",
    "    \n",
    "    def __str__(self):                      # String representation of a State object.\n",
    "        return f\"\"\"\n",
    "            own_pos (ft) = [{self.ownship_pos}]\n",
    "            own_vel (ft/s) = [{self.ownship_vel}]\n",
    "            int_pos (ft) = [{self.intruder_pos}]\n",
    "            int_vel (ft/s) = [{self.intruder_vel}]        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Initial State.\n",
    "**Creates an [ABSOLUTE STATE](#absolute_state) object from a given record of the [ENCOUNTER GEOMETRIES TABLE](#encounter_properties).**\n",
    "\n",
    "Assumptions:\n",
    "1. Ownship always heads north, $ v_{x_{0}} = 0$. \n",
    "\n",
    "2. Origin is always at the destination point $[0,0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeInitialState(encounter_properties: dict) -> State:\n",
    "    \"\"\"\n",
    "        Compute the ownship and intruder's initial states based on the encounter\n",
    "        design parameters:\n",
    "        encounter_properties is a dictionary as follows:\n",
    "            encounter_properties = {\n",
    "                0: (time_to_CPA),\n",
    "                1: (destination_time_after_CPA)\n",
    "                2: (OIF_CPA),\n",
    "                3: (CPA_distance_ft),\n",
    "                4: (v_o_kts)\n",
    "                5: (v_i_kts),\n",
    "                6: (int_rel_heading_deg),\n",
    "                7: (total_runs),\n",
    "                8: (depth),\n",
    "                9: (skip)\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        For the ownship:\n",
    "    \"\"\"\n",
    "    # Velocity of the ownship.\n",
    "    # Note: Ownship flights north so the x component of ownship_vel is 0.\n",
    "    ownship_vel_x = 0   # (ft/s).\n",
    "    speed_ownship = float(encounter_properties[4])\n",
    "    ownship_vel_y = speed_ownship * NMI_TO_FT / HR_TO_SEC   # (ft/s).\n",
    "    \n",
    "    # Velocity vector for the ownship. \n",
    "    ownship_vel = np.array([ownship_vel_x, ownship_vel_y])\n",
    "    \n",
    "    # Position of ownship.\n",
    "    # Ownship is placed south of the destination point [0,0].\n",
    "    ownship_x = 0\n",
    "    \n",
    "    # Using time to CPA and time to destination after CPA we can compute the initial y coordinates for the ownship.\n",
    "    time_to_CPA = float(encounter_properties[0])\n",
    "    destination_time_after_CPA = float(encounter_properties[1])\n",
    "    # Place the ownship at the correct y coordinate such that at the given velocity it will take \n",
    "    # (time_to_CPA + destination_time_after_CPA) seconds to reach the destination [0,0].\n",
    "    ownship_y = -(time_to_CPA + destination_time_after_CPA) * ownship_vel_y # (ft).\n",
    "    \n",
    "    # position vector (ownship).\n",
    "    ownship_pos = np.array([ownship_x, ownship_y])\n",
    "    \n",
    "    \"\"\"\n",
    "        For the intruder:\n",
    "    \"\"\"\n",
    "    # Velocity of the intruder:\n",
    "    \n",
    "    intruder_velocity_magnitud = float(encounter_properties[5]) * NMI_TO_FT / HR_TO_SEC     # (ft/s).\n",
    "    intruder_heading_angle = float(encounter_properties[6])     # degrees.\n",
    "    \n",
    "    intruder_vel_x = intruder_velocity_magnitud * math.sin(math.radians(intruder_heading_angle))\n",
    "    intruder_vel_y = intruder_velocity_magnitud * math.cos(math.radians(intruder_heading_angle))\n",
    "    \n",
    "    # velocity vector (intruder). \n",
    "    intruder_vel = np.array([intruder_vel_x, intruder_vel_y])\n",
    "    \n",
    "    # Position of the intruder:\n",
    "    \n",
    "    # Solve for initial position difference vector, delta_pos_t0:\n",
    "    delta_vel_t0 = intruder_vel - ownship_vel\n",
    "    delta_vel_magnitud = LA.norm(delta_vel_t0)\n",
    "    \n",
    "    # Get the horizontal distance at CPA.\n",
    "    S = float(encounter_properties[3])\n",
    "    # Initial distance btw aircraft:\n",
    "    delta_pos_magnitud = math.sqrt((S**2) + (time_to_CPA**2 * delta_vel_magnitud**2))\n",
    "    \n",
    "    # Let the angle between delta_vel_t0 and delta_post_t0 be theta.\n",
    "    cos_theta = -time_to_CPA * math.sqrt(delta_vel_magnitud**2) / delta_pos_magnitud\n",
    "    sin_theta_2 = 1 - cos_theta**2\n",
    "    \n",
    "    if sin_theta_2 < 0 and sin_theta_2 > -1e-15:\n",
    "        sin_theta_2 = 0\n",
    "    \n",
    "    sin_theta = math.sqrt(sin_theta_2)\n",
    "\n",
    "    # Given the angle we can rotate delta_vel_t0 to get delta_post_t0:\n",
    "    # Clock-wise rotation.\n",
    "    rotation_matrix = np.array([\n",
    "        [cos_theta, -sin_theta],\n",
    "        [sin_theta, cos_theta]\n",
    "    ])\n",
    "    delta_post_t0 = (rotation_matrix@delta_vel_t0)*delta_pos_magnitud/delta_vel_magnitud\n",
    "    \n",
    "    # Assume position of ownship is [0,0] then position of intruder is delta_post_t0.\n",
    "    # if x position of intruder at CPA is < 0 then ownship passes in front.\n",
    "    \n",
    "    # position vector (intruder).\n",
    "    intruder_pos = ownship_pos + delta_post_t0\n",
    "    intruder_pos_at_CPA = intruder_pos + (time_to_CPA*intruder_vel)\n",
    "    \n",
    "    OIF_CPA = bool(encounter_properties[2])\n",
    "    \n",
    "    if intruder_pos_at_CPA[0] <= 0 and OIF_CPA is True or intruder_pos_at_CPA[1] >= 0 and OIF_CPA is False:\n",
    "        # Rotation performed was the correct rotation.\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        # Perform Counter-clock wise rotation.\n",
    "        rotation_matrix = np.array([ \n",
    "            [cos_theta, sin_theta],\n",
    "            [-sin_theta,  cos_theta]\n",
    "        ])\n",
    "        delta_post_t0 = (rotation_matrix@delta_vel_t0)*delta_pos_magnitud/delta_vel_magnitud\n",
    "        intruder_pos = ownship_pos + delta_post_t0\n",
    "    \n",
    "    # Create State object\n",
    "    encounter_state = State(ownship_pos, intruder_pos, ownship_vel, intruder_vel)\n",
    "    return encounter_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute an initial state object given a directory for the training encounter csv file and an index for an encounter tuple in the table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInitStateFromEncounter(encounter_directory, encounter_index):\n",
    "    \"\"\"\n",
    "    Load the desc.csv file that contains the details about an encounter and get the initial state.\n",
    "\n",
    "    :param encounter_directory:\n",
    "    :param encounter_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load an encounter description from the directory.\n",
    "    ENCOUNTER_DESC = pd.read_csv(encounter_directory + '/desc.csv')\n",
    "\n",
    "    # Convert the encounter to a dictionary.\n",
    "    encounter_properties = ENCOUNTER_DESC.to_dict().get(str(encounter_index))\n",
    "\n",
    "    \"\"\"\n",
    "    Note:\n",
    "    encounter_properties is a dictionary with the following integer keys:\n",
    "        0: (time_to_CPA_sec) \n",
    "        1: (destination_time_after_CPA_sec) \n",
    "        2: (OIF_CPA) \n",
    "        3: (CPA_distance_ft)\n",
    "        4: (v_o_kts) \n",
    "        5: (v_i_kts) \n",
    "        6: (int_rel_heading_deg)\n",
    "    \"\"\"\n",
    "    # Given the encounter properties, compute the initial state of the system of the two aircraft.\n",
    "    encounter_state = computeInitialState(encounter_properties)\n",
    "    return encounter_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given a [ABSOLUTE STATE](#absolute_state) object and an action to take, compute the next state object after the action**<br>\n",
    "**Given state $q$ and action $a$ take $(q,a) -> q'$. Where $q'$ is a new state in continious space.\n",
    "Recall all rotation actions are $5$ deg/sec.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewState(state: State, action, TIME):\n",
    "    \"\"\"\n",
    "        Returns an instance of State which\n",
    "        represents a new state after taking an\n",
    "        action: (q,a) -> q'\n",
    "        TIME: How long should an action go for.\n",
    "\n",
    "    \"\"\"\n",
    "    ownship_vel = np.array(state.ownship_vel)\n",
    "    \n",
    "    # Velocity:\n",
    "    if action is 'NO_TURN':\n",
    "        new_vel_own = ownship_vel   # [v_x,v_y] (ft/sec).\n",
    "\n",
    "    else:\n",
    "        theta = 5   # degrees.\n",
    "        cos_theta = math.cos(math.radians(theta))\n",
    "        sin_theta = math.sin(math.radians(theta))\n",
    "        \n",
    "        if action is 'LEFT':\n",
    "            # Perform Counter-clock wise rotation.\n",
    "            rotation_matrix = np.array([ \n",
    "                [cos_theta, sin_theta],\n",
    "                [-sin_theta,  cos_theta]\n",
    "            ])\n",
    "            new_vel_own = rotation_matrix@ownship_vel\n",
    "\n",
    "        elif action is 'RIGHT':\n",
    "            # Perform clock-wise rotation.\n",
    "            rotation_matrix = np.array([\n",
    "                [cos_theta, -sin_theta],\n",
    "                [sin_theta, cos_theta]\n",
    "            ])\n",
    "            new_vel_own = rotation_matrix@ownship_vel\n",
    "    \n",
    "    # Position:\n",
    "    # For Ownship:\n",
    "    avg_disp = 0.5 * (new_vel_own + ownship_vel) * TIME\n",
    "    new_own_pos = state.ownship_pos + avg_disp  # [x_o,y_o] (ft).\n",
    "    \n",
    "    # For Intruder: Intruder flights at a constant velocity.\n",
    "    intr_vel = np.array(state.intruder_vel)\n",
    "    new_vel_intr = intr_vel\n",
    "    intr_disp = 0.5 * (new_vel_intr + intr_vel) * TIME\n",
    "    new_intr_pos = state.intruder_pos + intr_disp\n",
    "    \n",
    "    # New state after the action.\n",
    "    new_state = State(new_own_pos, new_intr_pos, new_vel_own, new_vel_intr)\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local State Definition\n",
    "<a id='local_state'></a>\n",
    "A Local State is a geometric representation of an Absolute State in continuous 2D coordinates. Converting an Absolute States to Local States allows us to take advantage of differnt forms of symmetry and simplify modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalState:\n",
    "    \"\"\"\n",
    "    A Local State is a geometrical representation of a given state in continuous\n",
    "    2D coordinates. Converting Absolute states to Local states allows us to take\n",
    "    advantage of symmetry.\n",
    "    \"\"\"\n",
    "    def __init__(self, r_do, theta_do, v_do, r_io, theta_io, psi_io_nr_io ,v_i ):\n",
    "        # Distance (ft) ownship to the destination.\n",
    "        self.distance_ownship_destination = r_do\n",
    "        # Angle ownship heading relative to destination.\n",
    "        self.theta_destintation_ownship = theta_do\n",
    "        # ownship velocity.\n",
    "        self.ownship_vel = v_do\n",
    "        # intruder velocity.\n",
    "        self.intruder_vel = v_i\n",
    "        # Distance between intruder and ownship.\n",
    "        self.distance_int_own = r_io\n",
    "        # Angle intruder heading relative to ownship heading.\n",
    "        self.theta_int_own_track = theta_io\n",
    "        self.angle_rel_vel_neg_rel_pos = psi_io_nr_io\n",
    "\n",
    "    # Return a string representation of a LocalState object.\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "            distance ownship destination (r_do) = {self.distance_ownship_destination},\n",
    "            angle destintation ownship (theta_do) = {self.theta_destintation_ownship},\n",
    "            ownship speed (v_do) = ({self.ownship_vel}),\n",
    "            distance intruder ownship (r_io) = ({self.distance_int_own}),\n",
    "            angle intruder ownship track (theta_io) = {self.theta_int_own_track},\n",
    "            angle of relative velocity w.r.t -(relative position) (psi_io_nr_io) = {self.angle_rel_vel_neg_rel_pos},\n",
    "            intruder speed (v_i) = {self.intruder_vel}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert an Absolute State to a Local State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAbsToLocal(absolute_encounter):\n",
    "    \"\"\"\n",
    "    Given an absolute state S, convert S to a local state L.\n",
    "    \"\"\"\n",
    "    # Ownship position [x, y]\n",
    "    ownship_pos = np.array(absolute_encounter.ownship_pos)\n",
    "    # Intruder position [x, y]\n",
    "    intruder_pos = np.array(absolute_encounter.intruder_pos)\n",
    "\n",
    "    # Ownship velocity [v_x, v_y]\n",
    "    ownship_vel = np.array(absolute_encounter.ownship_vel)\n",
    "    # Intruder velocity [v_x, v_y]\n",
    "    intruder_vel = np.array(absolute_encounter.intruder_vel)\n",
    "    \n",
    "    # Distance to the destination (ownship).\n",
    "    destination = np.array(DESTINATION_STATE)\n",
    "    dest_ownship_vector = destination - ownship_pos                 # [0,0] - [ownship_x, ownship_y].\n",
    "    distance_ownship_destination = LA.norm(dest_ownship_vector)     # distance to the destination at [0,0].\n",
    "\n",
    "    # atan2(y,x).\n",
    "    theta_destintation_ownship = math.degrees(math.atan2(dest_ownship_vector[1], dest_ownship_vector[0]))\n",
    "    \n",
    "    psi_o = math.degrees(math.atan2(ownship_vel[0], ownship_vel[1]))    # ownship vel w.r.t y axis.\n",
    "     \n",
    "    speed_destination_ownship = LA.norm(destination - ownship_vel)   # speed of ownship.\n",
    "    \n",
    "    intruder_pos_relative_ownship = intruder_pos - ownship_pos\n",
    "    distance_intruder_ownship = LA.norm(intruder_pos_relative_ownship)  # distance intruder and ownship.\n",
    "    \n",
    "    # intruder angle w.r.t y axis.\n",
    "    theta_int_own_orig = math.degrees(math.atan2(intruder_pos_relative_ownship[0], intruder_pos_relative_ownship[1]))\n",
    "    theta_intruder_own_track = theta_int_own_orig - psi_o # angle of the intruder pos w.r.t ownship's ground track.\n",
    "    \n",
    "    # tan^-1 (-180,180]\n",
    "    if theta_intruder_own_track < -180:\n",
    "        theta_intruder_own_track += 360\n",
    "    \n",
    "    elif theta_intruder_own_track > 180:\n",
    "        theta_intruder_own_track -= 360\n",
    "    \n",
    "    # speed of the intruder.\n",
    "    speed_intruder = LA.norm(intruder_vel)\n",
    "    \n",
    "    # Compute the angle between -intruder_pos_relative_ownship and intruder_vel_relative_ownship. \n",
    "    # This angle is 0 for a straight-to-collision geometry and increases in a clockwise fashion.\n",
    "    intruder_vel_relative_ownship = intruder_vel - ownship_vel\n",
    "    # w.r.t. the y-axis (-180, 180]\n",
    "    psi_io_rel_vel = math.degrees(math.atan2(intruder_vel_relative_ownship[0], intruder_vel_relative_ownship[1]))\n",
    "    \n",
    "    # angle of the psi_io_rel_vel vector w.r.t. the -intruder_pos_relative_ownship vector. \n",
    "    # This angle is 0 for a straight-to-collision geometry.\n",
    "    angle_rel_vel_neg_rel_pos = psi_io_rel_vel - (theta_int_own_orig - 180)\n",
    "    \n",
    "    if angle_rel_vel_neg_rel_pos <= -180:\n",
    "        angle_rel_vel_neg_rel_pos += 360\n",
    "    elif angle_rel_vel_neg_rel_pos > 180:\n",
    "        angle_rel_vel_neg_rel_pos -= 360\n",
    "        \n",
    "    # Create local state object.\n",
    "    local_state = LocalState(distance_ownship_destination,\n",
    "                             theta_destintation_ownship,\n",
    "                             speed_destination_ownship,\n",
    "                             distance_intruder_ownship,\n",
    "                             theta_intruder_own_track,\n",
    "                             angle_rel_vel_neg_rel_pos,\n",
    "                             speed_intruder)\n",
    "    return local_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is a local state in continuous space terminal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTerminalState(state: State):\n",
    "    \"\"\"\n",
    "    Is a Local State terminal?\n",
    "    Returns a non-zero reward for a final state:\n",
    "\n",
    "        DESTINATION_STATE_REWARD = 1.\n",
    "        ABANDON_STATE_REWARD = -0.5.\n",
    "        LODWC_REWARD = -0.3.\n",
    "\n",
    "    Otherwise return 0 for a non-final states.\n",
    "    \"\"\"\n",
    "    local_state = convertAbsToLocal(state)\n",
    "    \n",
    "    if local_state.distance_ownship_destination <= DESTINATION_DIST_ERROR:\n",
    "        return DESTINATION_STATE_REWARD     # Close enough to the destination, reward it.\n",
    "    if local_state.distance_ownship_destination > ABANDON_STATE_ERROR:\n",
    "        return ABANDON_STATE_REWARD     # Too far from destination, penalty.\n",
    "    if local_state.distance_int_own < DWC_DIST:\n",
    "        return LODWC_REWARD     # Lost of well clear.\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Search Tree\n",
    "## Select, Expand, Simulate, and Backpropagate\n",
    "Read more about how the Monte Carlo Tree Search works: [Monte Carlo Tree Search]('https://medium.com/@quasimik/monte-carlo-tree-search-applied-to-letterpress-34f41c86e238'). This will help to understand the implementation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MCTS](Images/MCTS.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCST_State:\n",
    "    \"\"\"\n",
    "        A MCTS State represents a node on the Monte Carlo Tree (MCT).\n",
    "        MCTS contains:\n",
    "            1. state: A state in continuous 2D.\n",
    "            2. Q: The average expected discounted sum of rewards from this node down the tree.\n",
    "            3. N: The number of times this node has been selected.\n",
    "            4. dirty_bit: Whether or not this node was updated on a given iteration of MCTS.\n",
    "    \"\"\"\n",
    "    def __init__(self, state: State):\n",
    "        # State properties\n",
    "        self.state = state\n",
    "        self.Q = 0\n",
    "        self.N = 0\n",
    "\n",
    "        # Dirty == 1 if this state was updated during simulations.\n",
    "        self.dirty_bit = 0\n",
    "\n",
    "        # Pointers to children states based on the available actions.\n",
    "        self.turn_left = None\n",
    "        self.turn_right = None\n",
    "        self.no_turn = None\n",
    "        # How many child states of this node have been expanded.\n",
    "        self.visited_child_count = 0\n",
    "\n",
    "    def updateQN(self, New_Q):\n",
    "        \"\"\"\n",
    "        When a node on the MCT is part of a path to a new expanded node the Q value of the new\n",
    "        expanded node via simulation is back-propagated to all the nodes that lead to it on the tree. This\n",
    "        propagated Q value is averaged with the current Q value of the node.\n",
    "        The N value is also increased by 1.\n",
    "        :param New_Q: The new Q value resulting from simulation.\n",
    "        \"\"\"\n",
    "        # First Time Update.\n",
    "        if self.N == 0:\n",
    "            self.Q = New_Q\n",
    "        else:  # Average with current Q value.\n",
    "            current_avg = self.Q\n",
    "            new_avg = current_avg + ((New_Q - current_avg) / (self.N + 1))\n",
    "            self.Q = new_avg\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Return a string represenation of a MCTS object.\n",
    "        \"\"\"\n",
    "        return f'''\n",
    "            STATE: {str(self.state)}\n",
    "            Q: {self.Q}\n",
    "            N: {self.N}\n",
    "        '''\n",
    "\n",
    "    def clean(self):\n",
    "        \"\"\"\n",
    "        Once we process an updated node, mark it as clean.\n",
    "        \"\"\"\n",
    "        self.dirty_bit = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCST:\n",
    "    \"\"\"\n",
    "    Implementation of the procedures needed to construct a MCTS: Selection, Expansion\n",
    "    Simulation, and Back-propagation.\n",
    "    \"\"\"\n",
    "    def __init__(self, state):\n",
    "        # Set the MCST initial state.\n",
    "        self.root = MCST_State(state)\n",
    "        self.root.N = 1\n",
    "        # Every iteration there is a sequence of selections that lead to an unknown state to be expanded. Keep track\n",
    "        self.visitedStatesPath = [self.root]    # Keep track of (state, action) pairs along the path to a final state.\n",
    "        # Points to the last expanded node on a given iteration.\n",
    "        self.lastExpandedState = self.root      # Reference to the last expanded node where simulation starts from.\n",
    "        # Sequence of  stats,action,rewards tuples to be used for the model: Refer to README for more details.\n",
    "        self.state_action_reward = []           # List of 3 elements tuples (state,action,reward).\n",
    "\n",
    "    def clearStatesPath(self):\n",
    "        \"\"\"\n",
    "        After processing the nodes, empty it for the next iteration of MCTS.\n",
    "        \"\"\"\n",
    "        self.visitedStatesPath = [self.root]\n",
    "\n",
    "    def getBestAction(self):\n",
    "        \"\"\"\n",
    "        The best action to take from this node is the one with the most simulations based on UCB1.\n",
    "        \"\"\"\n",
    "        simulations_count = [self.root.turn_left.N, self.root.no_turn.N, self.root.turn_right.N]\n",
    "        action_type = ['LEFT', 'NO_TURN', 'RIGHT']\n",
    "        action = action_type[simulations_count.index(max(simulations_count))]\n",
    "        \n",
    "        return action \n",
    "\n",
    "    def selection(self):\n",
    "        \"\"\"\n",
    "        Selection step on the MCTS iteration.\n",
    "        Starting  from the root, select the node with the highest UCB1 value.\n",
    "        \"\"\"\n",
    "        mcst_node = self.root\n",
    "\n",
    "        # We only run selection on nodes that have the 3 children expanded.\n",
    "        # While a given state node has been expanded, select a child using UCB1.\n",
    "        while mcst_node.visited_child_count == 3:   # LEFT, NO_TURN and RIGHT child states have been expanded.\n",
    "\n",
    "            # Exploration term:\n",
    "            c = UCB1_C\n",
    "\n",
    "            # Explore or exploit? UCB1 formula.\n",
    "            UCB1_left = mcst_node.turn_left.Q + c * math.sqrt((math.log(mcst_node.N) / mcst_node.turn_left.N))\n",
    "\n",
    "            UCB1_right = mcst_node.turn_right.Q + c * math.sqrt((math.log(mcst_node.N) / mcst_node.turn_right.N))\n",
    "\n",
    "            UCB1_no_turn = mcst_node.no_turn.Q + c * math.sqrt((math.log(mcst_node.N) / mcst_node.no_turn.N))\n",
    "\n",
    "            values = [UCB1_no_turn, UCB1_left, UCB1_right]\n",
    "            \n",
    "            nextChildIndex = values.index(max(UCB1_no_turn, UCB1_left, UCB1_right))\n",
    "\n",
    "            if nextChildIndex is 0:\n",
    "                # Select no_turn child.\n",
    "                mcst_node = mcst_node.no_turn\n",
    "            elif nextChildIndex is 1:\n",
    "                # Select left child.\n",
    "                mcst_node = mcst_node.turn_left\n",
    "            else:\n",
    "                # Select right child.\n",
    "                mcst_node = mcst_node.turn_right\n",
    "\n",
    "            # Add selected node to the Visited States Path.\n",
    "            self.visitedStatesPath.append(mcst_node)\n",
    "\n",
    "        # Return a selected node that does not have all 3 children node expanded: Used for expansion().\n",
    "        return mcst_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Discretizers: Bin continuous data into intervals.\n",
    "We cannot aim to model continuous space because it is computationally infeasible. Before we store a local state in our model, we discretize each of the features of the state to create a Discrete Local State.\n",
    "We use the KBinsDiscretizer provided by sklearn: [Check it out](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setUpdiscretizers():\n",
    "    \"\"\"\n",
    "    Generate and return a set of discretizers for every feature type.\n",
    "    Discretizer for Distance features.\n",
    "    Discretizer for Angle features.\n",
    "    Discretizer for Speed features.\n",
    "    :return: A set of discretizer objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the range of integer values for features: Defines all values to consider in discretization.\n",
    "    # Depends on the MAX and MIN values set in global_constants.py\n",
    "    range_distance = (np.array([[x for x in range(MIN_DISTANCE, MAX_DISTANCE + 1)]])).T\n",
    "    range_angle = (np.array([[x for x in range(MIN_ANGLE, MAX_ANGLE + 1)]])).T\n",
    "    range_speed = (np.array([[x for x in range(MIN_SPEED, MAX_SPEED + 1)]])).T\n",
    "\n",
    "    # Set the number of bins to use for every feature type.\n",
    "    \"\"\"\n",
    "        The number of bins used for every feature type directly influences the performance of the algorithm\n",
    "        both in training time (larger state space)  and quality of maneuvers.\n",
    "    \"\"\"\n",
    "    distance_bins = 121\n",
    "    angle_bins = 72\n",
    "    speed_bins = 57\n",
    "\n",
    "    # Generate the discretizer objects using KBinsDiscretizer module.\n",
    "    # Refer to sklearn KBinsDiscretizers documentation for discretizer types and options.\n",
    "    distance_discretizer = KBinsDiscretizer(n_bins=distance_bins, encode='ordinal', strategy='uniform')\n",
    "    angle_discretizer = KBinsDiscretizer(n_bins=angle_bins, encode='ordinal', strategy='uniform')\n",
    "    speed_discretizer = KBinsDiscretizer(n_bins=speed_bins, encode='ordinal', strategy='uniform')\n",
    "\n",
    "    # Fit the values for each range into bins using the discretization objects.\n",
    "    distance_discretizer.fit(range_distance)\n",
    "    angle_discretizer.fit(range_angle)\n",
    "    speed_discretizer.fit(range_speed)\n",
    "\n",
    "    # Compute the discrete state space size. Total of 7 features: 2 distance features, 3 angles features, 2 speed.\n",
    "    space_size = (distance_bins**2) * (angle_bins**3) * (speed_bins**2)\n",
    "\n",
    "    # Return the discretizers set to use them during training and testing.\n",
    "    return [distance_discretizer, angle_discretizer, speed_discretizer, space_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize a local state \n",
    "[Local State Definition](#local_state) <br>\n",
    "**A local state object is a 7 dimensional vector in continious space with the following features:**\n",
    "\n",
    "- distance_ownship_destination\n",
    "- theta_destintation_ownship\n",
    "- ownship_vel\n",
    "- intruder_vel\n",
    "- distance_int_own\n",
    "- theta_int_own_track\n",
    "- angle_rel_vel_neg_rel_pos\n",
    "\n",
    "**We discretize every feature to generate a 7 dimensinal discrete state object:**\n",
    "<a id='discrete_local_state'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteLocalState:\n",
    "    \"\"\"\n",
    "        Represents a local state after discretization.\n",
    "        Every feature is turned into a corresponding bin using the discretizers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_o_bin, d_i_o_bin, t_d_o_bin, t_i_o_bin, a_r_v_p_bin, o_v_bin, i_v_bin):\n",
    "        \"\"\"\n",
    "            Initialize the discrete features of this discrete state using the resulting\n",
    "            'bins' for every feature discretized in the continuous state space.\n",
    "        \"\"\"\n",
    "        # Distance ownship to destination bin\n",
    "        self.dis_ownship_destBIN = d_o_bin\n",
    "        # Distance angle to destination bin\n",
    "        self.theta_destintation_ownshipBIN = t_d_o_bin\n",
    "        # Distance ownship speed bin\n",
    "        self.ownship_velBIN = o_v_bin\n",
    "        # Distance intruder speed bin\n",
    "        self.intruder_velBIN = i_v_bin\n",
    "        # Distance intruder to ownship bin\n",
    "        self.dis_int_ownBIN = d_i_o_bin\n",
    "        # Angle intruder heading relative to ownship heading bin.\n",
    "        self.theta_int_own_trackBIN = t_i_o_bin\n",
    "        # TODO: Add desc.\n",
    "        self.angle_rel_vel_neg_rel_posBIN = a_r_v_p_bin\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "            A string representation of this discrete state.\n",
    "        \"\"\"\n",
    "        return f'''\n",
    "            d_o_bin = {self.dis_ownship_destBIN},\n",
    "            t_d_o_bin = {self.theta_destintation_ownshipBIN},\n",
    "            o_v_bin = {self.ownship_velBIN},\n",
    "            i_v_bin = {self.intruder_velBIN},\n",
    "            d_i_o_bin = {self.dis_int_ownBIN},\n",
    "            t_d_o_bin = {self.theta_int_own_trackBIN},\n",
    "            a_r_v_p_bin = {self.angle_rel_vel_neg_rel_posBIN}\n",
    "        '''\n",
    "\n",
    "    def __eq__(self, obj):\n",
    "        \"\"\"\n",
    "            Two discrete states are the same if they share all bins for discrete\n",
    "            features.\n",
    "        \"\"\"\n",
    "        if not isinstance(obj, DiscreteLocalState):\n",
    "            return False\n",
    "        if self.dis_ownship_destBIN != obj.dis_ownship_destBIN:\n",
    "            return False\n",
    "        if self.theta_destintation_ownshipBIN != obj.theta_destintation_ownshipBIN:\n",
    "            return False\n",
    "        if self.ownship_velBIN != obj.ownship_velBIN:\n",
    "            return False\n",
    "        if self.intruder_velBIN != obj.intruder_velBIN:\n",
    "            return False\n",
    "        if self.dis_int_ownBIN != obj.dis_int_ownBIN:\n",
    "            return False\n",
    "        if self.theta_int_own_trackBIN != obj.theta_int_own_trackBIN:\n",
    "            return False\n",
    "        if self.angle_rel_vel_neg_rel_posBIN != obj.angle_rel_vel_neg_rel_posBIN:\n",
    "            return False\n",
    "\n",
    "        # Discrete states are the same.\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize a state:\n",
    "Given a [Local State](#local_state) and the discretizers for every feature type (distance, angle, speed) generate a [Discrete Local State](#discrete_local_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretizeLocalState(local_state, distance_discretizer, angle_discretizer, speed_discretizer):\n",
    "    \"\"\"\n",
    "     Given a local state find the discretized versiob: Place every continuous feature into bins.\n",
    "    :param local_state: A local state to discretize.\n",
    "    :param distance_discretizer: The discretizer to use for distance features.\n",
    "    :param angle_discretizer: The discretizer to use for angle features.\n",
    "    :param speed_discretizer: The discretizer to use for speed features.\n",
    "    :return: A discrete state object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Vector set of distance features\n",
    "    LocalStateVectorDistances = [\n",
    "        local_state.distance_ownship_destination,\n",
    "        local_state.distance_int_own\n",
    "    ]\n",
    "    # Vector set of Angle features\n",
    "    LocalStateVectorAngles = [\n",
    "        local_state.theta_destintation_ownship,\n",
    "        local_state.theta_int_own_track,\n",
    "        local_state.angle_rel_vel_neg_rel_pos\n",
    "    ]\n",
    "    # Vector set of Speed features\n",
    "    LocalStateVectorSpeeds = [\n",
    "        local_state.ownship_vel,\n",
    "        local_state.intruder_vel\n",
    "    ]\n",
    "   \n",
    "    # Discretize features of the local state using the specified discretizers generated in setUpdiscretizers().\n",
    "    # Returns np.arrays with the bins.\n",
    "    distance_bins = distance_discretizer.transform((np.array([LocalStateVectorDistances])).T)\n",
    "    angle_bins = angle_discretizer.transform((np.array([LocalStateVectorAngles])).T)\n",
    "    speed_bins = speed_discretizer.transform((np.array([LocalStateVectorSpeeds])).T)\n",
    "\n",
    "    # distance ownship to destination bin\n",
    "    d_o_bin = distance_bins.T[0][0]\n",
    "    # Distance intruder to ownship bin\n",
    "    d_i_o_bin = distance_bins.T[0][1]\n",
    "\n",
    "    # Angle ownship heading to destination bin.\n",
    "    t_d_o_bin = angle_bins.T[0][0]\n",
    "    # Angle Intruder heading relative to ownship heading bin.\n",
    "    t_i_o_bin = angle_bins.T[0][1]\n",
    "    # TODO: Comment.\n",
    "    a_r_v_p_bin = angle_bins.T[0][2]\n",
    "\n",
    "    # Speed bins for ownship and intruder.\n",
    "    o_v_bin = speed_bins.T[0][0]\n",
    "    i_v_bin = speed_bins.T[0][1]\n",
    "\n",
    "    # Generate the discrete local state object.\n",
    "    discreteLocalState = DiscreteLocalState(d_o_bin, d_i_o_bin, t_d_o_bin, t_i_o_bin, a_r_v_p_bin, o_v_bin, i_v_bin)\n",
    "    \n",
    "    return discreteLocalState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we  have  Discrete Local States, our model will store the following:\n",
    "- A [Discrete Local State](#discrete_local_state)\n",
    "- The Q value for each action from this discrete local state.\n",
    "- The number of times N we have visited and updated this each action's Q value for this discrete local state in the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateActionQN:\n",
    "\n",
    "    def __init__(self, d_state: DiscreteLocalState, action, Q):\n",
    "        # Discrete State to be modeled by this object.\n",
    "        self.discrete_state = d_state\n",
    "        # The expected reward for taking a left turn.\n",
    "        self.LEFT_Q = 0\n",
    "        # The expected reward for going straight turn.\n",
    "        self.NO_TURN_Q = 0\n",
    "        # The expected reward for taking a right turn.\n",
    "        self.RIGHT_Q = 0\n",
    "\n",
    "        # Number of times I have visited this\n",
    "        self.NO_TURN_N = 0\n",
    "        self.LEFT_N = 0\n",
    "        self.RIGHT_N = 0\n",
    "\n",
    "        # Initial call to update the Q value of an action.\n",
    "        self.update(action, Q)\n",
    "    \n",
    "    def getBestAction(self):\n",
    "        \"\"\"\n",
    "        Return the action with the highest expected reward.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        actions = ['LEFT','NO_TURN','RIGHT']\n",
    "        # The set of Q values for the actions.\n",
    "        actionsQ = [self.LEFT_Q, self.NO_TURN_Q, self.RIGHT_Q]\n",
    "        \n",
    "        action = actions[actionsQ.index(max(actionsQ))]\n",
    "        return action\n",
    "    \n",
    "    def update(self, action: str, New_Q):\n",
    "        \"\"\"\n",
    "            Update our knowledge of this action\n",
    "            from this discrete state by averaging with its previous\n",
    "            Q value.\n",
    "        \"\"\"\n",
    "        if action is '':\n",
    "            return  \n",
    "        \n",
    "        if action is \"LEFT\":\n",
    "            \n",
    "            if self.LEFT_N == 0:\n",
    "                # First Q value for this action.\n",
    "                self.LEFT_Q = New_Q\n",
    "        \n",
    "            else:   # Average.\n",
    "                current_avg = self.LEFT_Q\n",
    "                new_avg = current_avg + ((New_Q - current_avg)/(self.LEFT_N+1))\n",
    "                self.LEFT_Q = new_avg\n",
    "\n",
    "            # Update number of visits to this action.\n",
    "            self.LEFT_N += 1\n",
    "        elif action is \"RIGHT\":\n",
    "\n",
    "            if self.RIGHT_N == 0:\n",
    "                # First Q value for this action.\n",
    "                self.RIGHT_Q = New_Q\n",
    "                \n",
    "            else:   # Average.\n",
    "                current_avg = self.RIGHT_Q\n",
    "                new_avg = current_avg + ((New_Q - current_avg)/(self.RIGHT_N+1))\n",
    "                self.RIGHT_Q = new_avg\n",
    "\n",
    "            # Update number of visits to this action.\n",
    "            self.RIGHT_N += 1\n",
    "\n",
    "        elif action is \"NO_TURN\":\n",
    "\n",
    "            if self.N == 0:\n",
    "                # First Q value for this action.\n",
    "                self.NO_TURN_Q = New_Q\n",
    "                \n",
    "            else:   # Average.\n",
    "                current_avg = self.NO_TURN_Q\n",
    "                new_avg = current_avg + ((New_Q - current_avg)/(self.NO_TURN_N+1))\n",
    "                self.NO_TURN_Q = new_avg\n",
    "\n",
    "            # Update number of visits to this action.\n",
    "            self.NO_TURN_N += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''\n",
    "            [discrete_state = {self.discrete_state}]\n",
    "            N(Updates) = {self.N}\n",
    "            LEFT_Q = {\"{:e}\".format(self.LEFT_Q)},\n",
    "            RIGHT_Q = {\"{:e}\".format(self.RIGHT_Q)},\n",
    "            NO_TURN_Q = {\"{:e}\".format(self.NO_TURN_Q)}\n",
    "        '''\n",
    "\n",
    "    def __eq__(self, obj):\n",
    "        return isinstance(obj, StateActionQN) and obj.discrete_state == self.discrete_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the code described above represents the heart of the PPA algorithm. The mechanics of training and testing are implemented in the source code PPA_Learn and PPA_Test, refer to README for instructions** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bitf491a08c665346ce8fbde46959170dd4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}